# yaml-language-server: $schema=../../../../schemas/agent_config_schema.yaml
AgentConfiguration:
  agent_name: evaluation_orchestrator_agent
  description: Evaluation Orchestrator Agent configuration
  config_version: '9.1'
  application_version: '1.1'
  application_name: llmops_workshop
  deployment:
    name: gpt-4o-mini
    model_name: gpt-4o-mini
    model_version: '2024-08-01'
    endpoint: https://<azure open ai instance name>.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-01-preview
    openai_api_version: 2024-10-01-preview
  model_parameters:
    temperature: 0.5
    seed: 42
  retrieval:
    parameters:
      search_type: hybrid
      top_k: 5
      index_name: micirosoft-tech-stack-index-0
      index_semantic_configuration_name: vector-llmops-workshop-index-semantic-configuration
    deployment:
      model_name: text-embedding-ada-002
      model_version: '2024-08-01'
      openai_api_version: 2024-10-01-preview
      name: text-embedding-ada-002
      endpoint: https://<azure open ai instance name>.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15
  system_prompt: > 
          Your task is to orchestrate LLM agents quality evaluation process.
          Your goal is to achive maximum qiality scoring in limited number of evaluation loops.
          You can use any available tools and agents to evaluate the quality of the LLM agents.
          You have following agents available:
          1. Prompt Generator
          2. Evaluation tool
          

